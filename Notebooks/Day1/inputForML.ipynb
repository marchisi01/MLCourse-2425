{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Day 1\n",
    "\n",
    "## You'll learn\n",
    "\n",
    "- familiarize with jupyter notebooks, numpy and pandas\n",
    "\n",
    "### Input data\n",
    "- efficient data format: convert CSV to Parquet\n",
    "- create input vector with features for MLLib. Format of the input depends on chosen ML library\n",
    "\n",
    "### Visualization\n",
    "- explore dataset, plot features\n",
    "- correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The dataset used in this example is described [here](https://archive.ics.uci.edu/ml/datasets/HIGGS). It is a binary classification problem where the goal is to train a classifier able to distinguish between a signal process, the production of new theoretical Higgs bosons, and a background process with identical decay products but distinct kinematic features.\n",
    "\n",
    "Each row of this dataset contains 28 features plus the label:\n",
    "\n",
    "- 21 low-level features which represent the basic measure made by the particle detector\n",
    "  - Momentum of the observed paricles\n",
    "  - Missing transverse momentum\n",
    "  - Jets and b-tagging information\n",
    "- 7 high-level features computed from the low-level features that encode the knowledge of the different intermediate states of the two processes (reconstructed invariant masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the execution environment\n",
    "\n",
    "Your code will run on a single dedicated server with 24 cores (hyperthreading enabled) and 192 GB of RAM. \n",
    "All the services needed for this tutorial are deployed as Kubernetes applications on this server. These include:\n",
    "* JupytherHub\n",
    "* Jupyter single-user servers\n",
    "* the HDFS file-system\n",
    "* Spark Clusters on demand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load custom magics definition\n",
    "\n",
    "We load an external file implemanting some custom *magics* function. Have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:11:30.394761Z",
     "iopub.status.busy": "2025-11-19T09:11:30.393702Z",
     "iopub.status.idle": "2025-11-19T09:11:30.422940Z",
     "shell.execute_reply": "2025-11-19T09:11:30.422205Z",
     "shell.execute_reply.started": "2025-11-19T09:11:30.394482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext custom_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:11:31.211783Z",
     "iopub.status.busy": "2025-11-19T09:11:31.211234Z",
     "iopub.status.idle": "2025-11-19T09:11:32.304687Z",
     "shell.execute_reply": "2025-11-19T09:11:32.303021Z",
     "shell.execute_reply.started": "2025-11-19T09:11:31.211721Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check out these custom functions\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Spark context\n",
    "\n",
    "We use the custom magic *%sc* to load a pre-defined Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:11:34.369588Z",
     "iopub.status.busy": "2025-11-19T09:11:34.368765Z",
     "iopub.status.idle": "2025-11-19T09:11:44.489168Z",
     "shell.execute_reply": "2025-11-19T09:11:44.487977Z",
     "shell.execute_reply.started": "2025-11-19T09:11:34.369519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-leggerf.jhub.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://192.168.2.39:6443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-leggerf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=k8s://https://192.168.2.39:6443 appName=jupyter-leggerf>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers=4\n",
    "spark=%sc $num_workers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession(spark)\n",
    "\n",
    "#check if spark is there\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:11:44.492149Z",
     "iopub.status.busy": "2025-11-19T09:11:44.491684Z",
     "iopub.status.idle": "2025-11-19T09:11:44.549081Z",
     "shell.execute_reply": "2025-11-19T09:11:44.548092Z",
     "shell.execute_reply.started": "2025-11-19T09:11:44.492109Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executors: 4\n",
      "Cores per executor: 5\n"
     ]
    }
   ],
   "source": [
    "# Check number of workers (executors), and cores per executor\n",
    "\n",
    "executor_count = len(spark._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "cores_per_executor = int(spark.getConf().get('spark.executor.cores','1'))\n",
    "\n",
    "print('Number of executors: '+ str(executor_count))\n",
    "print('Cores per executor: '+ str(cores_per_executor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "### Get familiar with kubernetes commands\n",
    "\n",
    "- You can open a terminal and use the commands: \n",
    "  - `kubectl get pods`\n",
    "  - `kubectl describe pod PODNAME`\n",
    "  - `kubectl get nodes`\n",
    "  - `kubectl describe node NODENAME`\n",
    "  - `kubectl describe farm`\n",
    "  - ```kubectl logs PODNAME```\n",
    "  - ```kubectl exec PODNAME -it -- /bin/bash```\n",
    "  \n",
    "  \n",
    "- Try to stop spark with `spark.stop()`, and start it again running the cell above with a different number of workers. What happens? You can play with the number of workers, and run the cells below that execute spark commands. Provided you're getting all the workers you're asking for, does the execution time change? Try to make some scaling tests\n",
    "\n",
    "- if you don't stop spark correctly, you will see pods in Error state. You can get rid of those by running this command:\n",
    "    `kubectl get pods -n YOURUSERNAME | grep Error | awk '{print $1}' | xargs kubectl delete pod -n YOURUSERNAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:12:33.460468Z",
     "iopub.status.busy": "2025-11-19T09:12:33.459787Z",
     "iopub.status.idle": "2025-11-19T09:12:34.709602Z",
     "shell.execute_reply": "2025-11-19T09:12:34.707793Z",
     "shell.execute_reply.started": "2025-11-19T09:12:33.460378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         jupyter-leggerf-dda6179a9b623b67-exec-1\n",
      "Namespace:    leggerf\n",
      "Priority:     0\n",
      "Node:         t2-mlwn-04.to.infn.it/192.168.2.84\n",
      "Start Time:   Wed, 19 Nov 2025 09:13:17 +0000\n",
      "Labels:       spark-app-selector=spark-application-1763543498158\n",
      "              spark-exec-id=1\n",
      "              spark-exec-resourceprofile-id=0\n",
      "              spark-role=executor\n",
      "Annotations:  cni.projectcalico.org/containerID: 0dde9a18c602e7ab667917943279c8a5fc0bed8f58e8f2c918963ba8ce7e338e\n",
      "              cni.projectcalico.org/podIP: 192.168.230.110/32\n",
      "              cni.projectcalico.org/podIPs: 192.168.230.110/32\n",
      "Status:       Running\n",
      "IP:           192.168.230.110\n",
      "IPs:\n",
      "  IP:  192.168.230.110\n",
      "Containers:\n",
      "  spark-kubernetes-executor:\n",
      "    Container ID:  docker://59534f7effe1f620f429cfd11057ca4caaec360b6719a7198218773d68c178b9\n",
      "    Image:         svallero/sparkpy:3.2.1\n",
      "    Image ID:      docker-pullable://svallero/sparkpy@sha256:3590e42c83c450d00b3c02a19c84903759920e6f58e46072f090e8a18040b760\n",
      "    Port:          7079/TCP\n",
      "    Host Port:     0/TCP\n",
      "    Args:\n",
      "      executor\n",
      "    State:          Running\n",
      "      Started:      Wed, 19 Nov 2025 09:13:20 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:  11Gi\n",
      "    Requests:\n",
      "      cpu:     5\n",
      "      memory:  11Gi\n",
      "    Environment:\n",
      "      SPARK_USER:                 jovyan\n",
      "      SPARK_DRIVER_URL:           spark://CoarseGrainedScheduler@jupyter-leggerf.jhub.svc.cluster.local:34782\n",
      "      SPARK_EXECUTOR_CORES:       5\n",
      "      SPARK_EXECUTOR_MEMORY:      10240m\n",
      "      SPARK_APPLICATION_ID:       spark-application-1763543498158\n",
      "      SPARK_CONF_DIR:             /opt/spark/conf\n",
      "      SPARK_EXECUTOR_ID:          1\n",
      "      SPARK_RESOURCE_PROFILE_ID:  0\n",
      "      HADOOP_USER_NAME:           jovyan\n",
      "      SPARK_EXECUTOR_POD_IP:       (v1:status.podIP)\n",
      "      SPARK_JAVA_OPT_0:           -Dio.netty.tryReflectionSetAccessible=true\n",
      "      SPARK_JAVA_OPT_1:           -Dspark.driver.port=34782\n",
      "      SPARK_LOCAL_DIRS:           /var/data/spark-cf49f462-42cd-4a38-9f66-58714a4adc7b\n",
      "    Mounts:\n",
      "      /data-corso from data-corso (ro)\n",
      "      /home/jovyan from home (rw)\n",
      "      /opt/spark/conf from spark-conf-volume-exec (rw)\n",
      "      /var/data/spark-cf49f462-42cd-4a38-9f66-58714a4adc7b from spark-local-dir-1 (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8wldq (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  spark-conf-volume-exec:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      spark-exec-f57e869a9b623e75-conf-map\n",
      "    Optional:  false\n",
      "  home:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /glusterstorage/volume-leggerf-mlwn\n",
      "    HostPathType:  \n",
      "  data-corso:\n",
      "    Type:          HostPath (bare host directory volume)\n",
      "    Path:          /glusterstorage/volume-data-corso\n",
      "    HostPathType:  \n",
      "  spark-local-dir-1:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     \n",
      "    SizeLimit:  <unset>\n",
      "  kube-api-access-8wldq:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/memory-pressure:NoSchedule op=Exists\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type    Reason     Age        From               Message\n",
      "  ----    ------     ----       ----               -------\n",
      "  Normal  Scheduled  55s        default-scheduler  Successfully assigned leggerf/jupyter-leggerf-dda6179a9b623b67-exec-1 to t2-mlwn-04.to.infn.it\n",
      "  Normal  Pulled     <invalid>  kubelet            Container image \"svallero/sparkpy:3.2.1\" already present on machine\n",
      "  Normal  Created    <invalid>  kubelet            Created container spark-kubernetes-executor\n",
      "  Normal  Started    <invalid>  kubelet            Started container spark-kubernetes-executor\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod jupyter-leggerf-dda6179a9b623b67-exec-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "### Create parquet files, which are faster to read than CSV\n",
    "\n",
    "- create a parquet file for all input files, you will need them for the next notebooks\n",
    "- check you can read in all parquet files and that they have the correct number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read input file\n",
    "inputFile = 'file:///data-corso/Higgs100k.csv'\n",
    "#inputFile = 'file:///data-corso/Higgs1M.csv'\n",
    "#inputFile = 'file:///data-corso/Higgs10M.csv'\n",
    "\n",
    "%time df = spark_session.read.format('csv').option('header', 'true').option('inferschema', 'true').load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#write parquet in your home\n",
    "outputFile = 'file:///home/jovyan/Higgs100k.parquet'\n",
    "#outputFile = 'file:///home/jovyan/Higgs1M.parquet'\n",
    "#outputFile = 'file:///home/jovyan/Higgs10M.parquet'\n",
    "\n",
    "df.write.parquet(outputFile, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in parquet files\n",
    "\n",
    "how much faster is it to read parquet files rather than csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputFile = 'file:///home/jovyan/Higgs100k.parquet'\n",
    "#inputFile = 'file:///home/jovyan/Higgs1M.parquet'\n",
    "#inputFile = 'file:///home/jovyan/Higgs10M.parquet'\n",
    "\n",
    "%time df = spark_session.read.format('parquet').option('header', 'true').option('inferschema', 'true').load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In how many partitions is the dataframe distributed?\n",
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df.rdd.partitioner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's have a look at the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time total_events = df.count()\n",
    "\n",
    "print('There are '+str(total_events)+' events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "- Create a function that makes a plot of any of the above variable for signal versus background (using the label variable to discriminate)\n",
    "  - see an example of the plot in the hands-on slides\n",
    "  - the function should take as input the dataframe *df* and the variable name. For example `plotSignalvsBg(df, 'm_bb')`\n",
    "  - to develop the code, use the 100k dataset, so that debugging goes quicker\n",
    "- try to plot a few input variables and try to understand which ones are more promising to distinguish signal from background  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 4 - Bonus\n",
    "\n",
    "### Create the input feature vector\n",
    "\n",
    "- Libraries for ML tipically take as inputs data in a very specific format. Documentation on how to do data preprocessing in Spark: https://spark.apache.org/docs/latest/ml-features.html\n",
    "- Try to add to the dataframe df a new column, called 'features' which is a vector column with all the variables above except for 'label'\n",
    "   - features = [lepton_pT, lepton_eta, lepton_phi, ...]\n",
    "   - Hint: look at the VectorAssembler transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Bonus\n",
    "\n",
    "Print (or draw) the correlation matrix (a table showing correlation coefficients between variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when you're done, stop spark, this will release the resources you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:13:14.650328Z",
     "iopub.status.busy": "2025-11-19T09:13:14.649663Z",
     "iopub.status.idle": "2025-11-19T09:13:15.367719Z",
     "shell.execute_reply": "2025-11-19T09:13:15.366238Z",
     "shell.execute_reply.started": "2025-11-19T09:13:14.650251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get pods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.maxResultSize",
     "value": "0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
